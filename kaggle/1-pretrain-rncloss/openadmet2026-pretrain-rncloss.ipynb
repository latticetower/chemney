{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":292603417,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we use DeepChem/ChemBERTa-77M-MLM as a baseline and uses https://github.com/kaiwenzha/Rank-N-Contrast as a loss","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb_key\")\n    wandb.login(key=wandb_key)\n    wandb.init(entity='lacemaker', project='openadmet2026')\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:49:55.643162Z","iopub.execute_input":"2026-01-19T02:49:55.643429Z","iopub.status.idle":"2026-01-19T02:50:02.430647Z","shell.execute_reply.started":"2026-01-19T02:49:55.643405Z","shell.execute_reply":"2026-01-19T02:50:02.430098Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-gorge-4</strong> at: <a href='https://wandb.ai/lacemaker/openadmet2026/runs/edyeoy3v' target=\"_blank\">https://wandb.ai/lacemaker/openadmet2026/runs/edyeoy3v</a><br> View project at: <a href='https://wandb.ai/lacemaker/openadmet2026' target=\"_blank\">https://wandb.ai/lacemaker/openadmet2026</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20260119_024949-edyeoy3v/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20260119_024955-iyvmnug5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lacemaker/openadmet2026/runs/iyvmnug5' target=\"_blank\">generous-disco-5</a></strong> to <a href='https://wandb.ai/lacemaker/openadmet2026' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lacemaker/openadmet2026' target=\"_blank\">https://wandb.ai/lacemaker/openadmet2026</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lacemaker/openadmet2026/runs/iyvmnug5' target=\"_blank\">https://wandb.ai/lacemaker/openadmet2026/runs/iyvmnug5</a>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nOUTPUTDIR = Path(\"../working\")\nOUTPUTDIR.mkdir(exist_ok=True)\nWHEELDIR = (OUTPUTDIR / \"wheels\").as_posix()\nREQUIREMENTS = (OUTPUTDIR/\"requirements.txt\").as_posix()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:50:02.434204Z","iopub.execute_input":"2026-01-19T02:50:02.434416Z","iopub.status.idle":"2026-01-19T02:50:02.438416Z","shell.execute_reply.started":"2026-01-19T02:50:02.434394Z","shell.execute_reply":"2026-01-19T02:50:02.437759Z"},"_kg_hide-input":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%writefile $REQUIREMENTS\n#scikit-learn\n#sklearn-compat\n#category-encoders\n#cesium\neinops\nsentence-transformers # == 5.1.0\ntorch # == 2.6.0 --index-url https://download.pytorch.org/whl/cu128\ntabpfn\n#transformers\n# rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:50:02.439139Z","iopub.execute_input":"2026-01-19T02:50:02.439341Z","iopub.status.idle":"2026-01-19T02:50:02.453199Z","shell.execute_reply.started":"2026-01-19T02:50:02.439320Z","shell.execute_reply":"2026-01-19T02:50:02.452535Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Overwriting ../working/requirements.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:50:02.454082Z","iopub.execute_input":"2026-01-19T02:50:02.454342Z","iopub.status.idle":"2026-01-19T02:50:02.630760Z","shell.execute_reply.started":"2026-01-19T02:50:02.454320Z","shell.execute_reply":"2026-01-19T02:50:02.630117Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Mon Jan 19 02:50:02 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   40C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip download --destination-directory $WHEELDIR -r $REQUIREMENTS\n!pip wheel --wheel-dir $WHEELDIR -r $REQUIREMENTS\n!pip install --upgrade --no-index --find-links=$WHEELDIR -r $REQUIREMENTS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:50:02.632021Z","iopub.execute_input":"2026-01-19T02:50:02.632328Z","iopub.status.idle":"2026-01-19T02:51:15.523139Z","shell.execute_reply.started":"2026-01-19T02:50:02.632296Z","shell.execute_reply":"2026-01-19T02:51:15.522458Z"},"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Collecting einops (from -r ../working/requirements.txt (line 5))\n  File was already downloaded /kaggle/working/wheels/einops-0.8.1-py3-none-any.whl\nCollecting sentence-transformers (from -r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/sentence_transformers-5.2.0-py3-none-any.whl\nCollecting torch (from -r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl\nCollecting tabpfn (from -r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tabpfn-6.3.1-py3-none-any.whl\nCollecting transformers<6.0.0,>=4.41.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/transformers-4.57.6-py3-none-any.whl\nCollecting tqdm (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/tqdm-4.67.1-py3-none-any.whl\nCollecting scikit-learn (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\nCollecting scipy (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\nCollecting typing_extensions>=4.5.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/typing_extensions-4.15.0-py3-none-any.whl\nCollecting filelock (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/filelock-3.20.3-py3-none-any.whl\nCollecting setuptools (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/setuptools-80.9.0-py3-none-any.whl\nCollecting sympy>=1.13.3 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/sympy-1.14.0-py3-none-any.whl\nCollecting networkx>=2.5.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/networkx-3.6.1-py3-none-any.whl\nCollecting jinja2 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/jinja2-3.1.6-py3-none-any.whl\nCollecting fsspec>=0.8.5 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/fsspec-2026.1.0-py3-none-any.whl\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl\nCollecting nvidia-nccl-cu12==2.27.5 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvshmem-cu12==3.3.20 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting triton==3.5.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting numpy<3,>=1.21.6 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting scikit-learn (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting pandas<3,>=1.4.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\nCollecting pydantic>=2.8.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic-2.12.5-py3-none-any.whl\nCollecting pydantic-settings>=2.10.1 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic_settings-2.12.0-py3-none-any.whl\nCollecting eval-type-backport>=0.2.2 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/eval_type_backport-0.3.1-py3-none-any.whl\nCollecting joblib>=1.2.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/joblib-1.5.3-py3-none-any.whl\nCollecting tabpfn-common-utils>=0.2.13 (from tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tabpfn_common_utils-0.2.14-py3-none-any.whl\nCollecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/packaging-25.0-py3-none-any.whl\nCollecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\nCollecting python-dateutil>=2.8.2 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\nCollecting pytz>=2020.1 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pytz-2025.2-py2.py3-none-any.whl\nCollecting tzdata>=2022.7 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tzdata-2025.3-py2.py3-none-any.whl\nCollecting annotated-types>=0.6.0 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/annotated_types-0.7.0-py3-none-any.whl\nCollecting pydantic-core==2.41.5 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting typing-inspection>=0.4.2 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/typing_inspection-0.4.2-py3-none-any.whl\nCollecting python-dotenv>=0.21.0 (from pydantic-settings>=2.10.1->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/python_dotenv-1.2.1-py3-none-any.whl\nCollecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/threadpoolctl-3.6.0-py3-none-any.whl\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/mpmath-1.3.0-py3-none-any.whl\nCollecting platformdirs>=4 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/platformdirs-4.5.1-py3-none-any.whl\nCollecting posthog~=6.7 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/posthog-6.9.3-py3-none-any.whl\nCollecting requests>=2.32.5 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/requests-2.32.5-py3-none-any.whl\nCollecting ruff>=0.11.6 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/ruff-0.14.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/huggingface_hub-0.36.0-py3-none-any.whl\nCollecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting MarkupSafe>=2.0 (from jinja2->torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting six>=1.5 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/six-1.17.0-py2.py3-none-any.whl\nCollecting backoff>=1.10.0 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/backoff-2.2.1-py3-none-any.whl\nCollecting distro>=1.5.0 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/distro-1.9.0-py3-none-any.whl\nCollecting charset_normalizer<4,>=2 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting idna<4,>=2.5 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/idna-3.11-py3-none-any.whl\nCollecting urllib3<3,>=1.21.1 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/urllib3-2.6.3-py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/certifi-2026.1.4-py3-none-any.whl\nCollecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\nSuccessfully downloaded einops sentence-transformers torch nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-cufile-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-cusparselt-cu12 nvidia-nccl-cu12 nvidia-nvjitlink-cu12 nvidia-nvshmem-cu12 nvidia-nvtx-cu12 triton tabpfn eval-type-backport fsspec joblib networkx numpy pandas pydantic pydantic-core pydantic-settings scikit-learn scipy sympy tabpfn-common-utils filelock tqdm transformers huggingface-hub typing_extensions jinja2 setuptools annotated-types hf-xet MarkupSafe mpmath packaging platformdirs posthog python-dateutil python-dotenv pytz pyyaml regex requests ruff safetensors threadpoolctl tokenizers typing-inspection tzdata backoff certifi charset_normalizer distro idna six urllib3\nCollecting einops (from -r ../working/requirements.txt (line 5))\n  File was already downloaded /kaggle/working/wheels/einops-0.8.1-py3-none-any.whl\nCollecting sentence-transformers (from -r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/sentence_transformers-5.2.0-py3-none-any.whl\nCollecting torch (from -r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl\nCollecting tabpfn (from -r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tabpfn-6.3.1-py3-none-any.whl\nCollecting transformers<6.0.0,>=4.41.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/transformers-4.57.6-py3-none-any.whl\nCollecting tqdm (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/tqdm-4.67.1-py3-none-any.whl\nCollecting scikit-learn (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\nCollecting scipy (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\nCollecting typing_extensions>=4.5.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/typing_extensions-4.15.0-py3-none-any.whl\nCollecting filelock (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/filelock-3.20.3-py3-none-any.whl\nCollecting setuptools (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/setuptools-80.9.0-py3-none-any.whl\nCollecting sympy>=1.13.3 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/sympy-1.14.0-py3-none-any.whl\nCollecting networkx>=2.5.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/networkx-3.6.1-py3-none-any.whl\nCollecting jinja2 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/jinja2-3.1.6-py3-none-any.whl\nCollecting fsspec>=0.8.5 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/fsspec-2026.1.0-py3-none-any.whl\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl\nCollecting nvidia-nccl-cu12==2.27.5 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvshmem-cu12==3.3.20 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting triton==3.5.1 (from torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting numpy<3,>=1.21.6 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting scikit-learn (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting pandas<3,>=1.4.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\nCollecting pydantic>=2.8.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic-2.12.5-py3-none-any.whl\nCollecting pydantic-settings>=2.10.1 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic_settings-2.12.0-py3-none-any.whl\nCollecting eval-type-backport>=0.2.2 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/eval_type_backport-0.3.1-py3-none-any.whl\nCollecting joblib>=1.2.0 (from tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/joblib-1.5.3-py3-none-any.whl\nCollecting tabpfn-common-utils>=0.2.13 (from tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tabpfn_common_utils-0.2.14-py3-none-any.whl\nCollecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/packaging-25.0-py3-none-any.whl\nCollecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\nCollecting python-dateutil>=2.8.2 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\nCollecting pytz>=2020.1 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pytz-2025.2-py2.py3-none-any.whl\nCollecting tzdata>=2022.7 (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/tzdata-2025.3-py2.py3-none-any.whl\nCollecting annotated-types>=0.6.0 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/annotated_types-0.7.0-py3-none-any.whl\nCollecting pydantic-core==2.41.5 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting typing-inspection>=0.4.2 (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/typing_inspection-0.4.2-py3-none-any.whl\nCollecting python-dotenv>=0.21.0 (from pydantic-settings>=2.10.1->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/python_dotenv-1.2.1-py3-none-any.whl\nCollecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/threadpoolctl-3.6.0-py3-none-any.whl\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/mpmath-1.3.0-py3-none-any.whl\nCollecting platformdirs>=4 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/platformdirs-4.5.1-py3-none-any.whl\nCollecting posthog~=6.7 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/posthog-6.9.3-py3-none-any.whl\nCollecting requests>=2.32.5 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/requests-2.32.5-py3-none-any.whl\nCollecting ruff>=0.11.6 (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/ruff-0.14.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/huggingface_hub-0.36.0-py3-none-any.whl\nCollecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  File was already downloaded /kaggle/working/wheels/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting MarkupSafe>=2.0 (from jinja2->torch->-r ../working/requirements.txt (line 7))\n  File was already downloaded /kaggle/working/wheels/markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting six>=1.5 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/six-1.17.0-py2.py3-none-any.whl\nCollecting backoff>=1.10.0 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/backoff-2.2.1-py3-none-any.whl\nCollecting distro>=1.5.0 (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/distro-1.9.0-py3-none-any.whl\nCollecting charset_normalizer<4,>=2 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\nCollecting idna<4,>=2.5 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/idna-3.11-py3-none-any.whl\nCollecting urllib3<3,>=1.21.1 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/urllib3-2.6.3-py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests>=2.32.5->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8))\n  File was already downloaded /kaggle/working/wheels/certifi-2026.1.4-py3-none-any.whl\nCollecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6))\n  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\nLooking in links: ../working/wheels\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r ../working/requirements.txt (line 5)) (0.8.1)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r ../working/requirements.txt (line 6)) (5.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r ../working/requirements.txt (line 7)) (2.9.1)\nRequirement already satisfied: tabpfn in /usr/local/lib/python3.12/dist-packages (from -r ../working/requirements.txt (line 8)) (6.3.1)\nRequirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (0.36.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r ../working/requirements.txt (line 6)) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (1.13.1.3)\nRequirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r ../working/requirements.txt (line 7)) (3.5.1)\nRequirement already satisfied: numpy<3,>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (2.0.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (2.2.2)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (2.12.5)\nRequirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (2.11.0)\nRequirement already satisfied: eval-type-backport>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (0.3.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn->-r ../working/requirements.txt (line 8)) (1.5.3)\nRequirement already satisfied: tabpfn-common-utils>=0.2.13 in /usr/local/lib/python3.12/dist-packages (from tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (0.2.14)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (1.2.1rc0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn->-r ../working/requirements.txt (line 8)) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8)) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn->-r ../working/requirements.txt (line 8)) (0.4.2)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.10.1->tabpfn->-r ../working/requirements.txt (line 8)) (1.1.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r ../working/requirements.txt (line 6)) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r ../working/requirements.txt (line 7)) (1.3.0)\nRequirement already satisfied: platformdirs>=4 in /usr/local/lib/python3.12/dist-packages (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (4.5.1)\nRequirement already satisfied: posthog~=6.7 in /usr/local/lib/python3.12/dist-packages (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (6.9.3)\nRequirement already satisfied: ruff>=0.11.6 in /usr/local/lib/python3.12/dist-packages (from tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (0.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r ../working/requirements.txt (line 7)) (3.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (1.17.0)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.13->tabpfn-common-utils[telemetry-interactive]>=0.2.13->tabpfn->-r ../working/requirements.txt (line 8)) (1.9.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->-r ../working/requirements.txt (line 6)) (2026.1.4)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport shutil\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:51:42.630518Z","iopub.execute_input":"2026-01-19T02:51:42.631199Z","iopub.status.idle":"2026-01-19T02:51:43.057962Z","shell.execute_reply.started":"2026-01-19T02:51:42.631162Z","shell.execute_reply":"2026-01-19T02:51:43.057242Z"}},"outputs":[{"name":"stdout","text":"../input/openadmet2026-data-split/train_folds.csv\n../input/openadmet2026-data-split/test_with_augmentations.csv\n../input/openadmet2026-data-split/__results__.html\n../input/openadmet2026-data-split/__notebook__.ipynb\n../input/openadmet2026-data-split/__output__.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import sentence_transformers as st\nst.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:51:46.735363Z","iopub.execute_input":"2026-01-19T02:51:46.735884Z","execution_failed":"2026-01-19T02:51:53.508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set as {seed}\")\n\nRANDOM_SEED = 3407  # 42  # 3407\nset_seed(RANDOM_SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"import kagglehub\nopenadmet_data_split_path = kagglehub.notebook_output_download('latticetower/openadmet2026-data-split')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(Path(openadmet_data_split_path) / \"train_folds.csv\")\nblind_test_df = pd.read_csv(Path(openadmet_data_split_path) / \"test_with_augmentations.csv\")\n# fold_df = pd.read_csv(Path(latticetower_polymers_data_split_ext_path) / \"train_folds.csv\")\nadditional_smiles_columns = [col for col in train_df.columns if col.startswith('AUG_SMILES')]\n\nmerged_df = train_df\nprint(merged_df.shape)\nmerged_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:53:05.870372Z","iopub.execute_input":"2026-01-19T02:53:05.870666Z","iopub.status.idle":"2026-01-19T02:53:05.881484Z","shell.execute_reply.started":"2026-01-19T02:53:05.870640Z","shell.execute_reply":"2026-01-19T02:53:05.880483Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_384/1187713033.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenadmet_data_split_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"train_folds.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mblind_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenadmet_data_split_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"test_with_augmentations.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fold_df = pd.read_csv(Path(latticetower_polymers_data_split_ext_path) / \"train_folds.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madditional_smiles_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AUG_SMILES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"],"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"blind_test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ids = merged_df.fold_name.isin(['train', 'test', 'val'])\n\ntrain_df = merged_df.loc[train_ids].reset_index(drop=True)\nval_df = merged_df.loc[merged_df.fold_name == 'val'].reset_index(drop=True)\ntest_df = merged_df.loc[merged_df.fold_name == 'test'].reset_index(drop=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET_COLUMNS = [\n    'LogD', 'KSOL', 'HLM CLint', 'MLM CLint',\n    'Caco-2 Permeability Papp A>B', 'Caco-2 Permeability Efflux', 'MPPB',\n    'MBPB', 'MGMB'\n]\n\nMODEL_NAME = \"DeepChem/ChemBERTa-77M-MLM\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define model","metadata":{}},{"cell_type":"code","source":"%%writefile mixer_wrapper.py\nfrom typing import Callable, Self\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport sentence_transformers as st\nfrom sentence_transformers.models import Module\nfrom einops.layers.torch import EinMix as Mix\n\n\n# https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Dense.py#L16-L105\n# https://github.com/UKPLab/sentence-transformers/blob/v4.1.0/sentence_transformers/models/Dense.py#L15\nclass MixerWrapper(Module):\n    config_keys: list[str] = [\n        \"in_features\",\n        \"out_features\",\n        \"n_channels\",\n        \"activation_function\",\n    ]\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        n_channels=5,\n        activation_function: Callable[[Tensor], Tensor] | None = nn.Tanh(),\n        **kwargs\n    ) -> None:\n\n        super(MixerWrapper, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_channels = n_channels\n        self.activation_function = nn.Identity() \\\n            if activation_function is None \\\n            else activation_function\n\n        self.mixer_layer = Mix(\n            \"b e -> b o ch\",\n            weight_shape=\"e o ch\",\n            bias_shape=\"o ch\",\n            e=in_features,\n            ch=n_channels,\n            o=out_features\n        )\n\n    def forward(self, features: dict[str, torch.Tensor], **kwargs) -> dict[str, torch.Tensor]:\n        features.update({\n            \"sentence_embedding\": self.activation_function(\n                self.mixer_layer(features[\"sentence_embedding\"]))\n        })\n        return features\n\n    def get_sentence_embedding_dimension(self) -> int:\n        return self.out_features\n\n    def get_config_dict(self):\n        return {\n            \"in_features\": self.in_features,\n            \"out_features\": self.out_features,\n            \"n_channels\": self.n_channels,\n            \"activation_function\": st.util.misc.fullname(self.activation_function),\n        }\n    def save(self, output_path: str, *args, safe_serialization: bool = True, **kwargs) -> None:\n        self.save_config(output_path)\n        self.save_torch_weights(output_path, safe_serialization=safe_serialization)\n\n    def __repr__(self):\n        return f\"MixerWrapper({self.get_config_dict()})\"\n\n    @classmethod\n    def load(\n        cls,\n        model_name_or_path: str,\n        subfolder: str = \"\",\n        token: bool | str | None = None,\n        cache_folder: str | None = None,\n        revision: str | None = None,\n        local_files_only: bool = False,\n        **kwargs,\n    ) -> Self:\n        hub_kwargs = {\n            \"subfolder\": subfolder,\n            \"token\": token,\n            \"cache_folder\": cache_folder,\n            \"revision\": revision,\n            \"local_files_only\": local_files_only,\n        }\n        config = cls.load_config(model_name_or_path=model_name_or_path, **hub_kwargs)\n        config[\"activation_function\"] = st.util.misc.import_from_string(config[\"activation_function\"])()\n        model = cls(**config)\n        model = cls.load_torch_weights(\n            model_name_or_path=model_name_or_path,\n            model=model,\n            **hub_kwargs\n        )\n        return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:59:02.181642Z","iopub.execute_input":"2026-01-19T02:59:02.181978Z","iopub.status.idle":"2026-01-19T02:59:02.189040Z","shell.execute_reply.started":"2026-01-19T02:59:02.181948Z","shell.execute_reply":"2026-01-19T02:59:02.188261Z"}},"outputs":[{"name":"stdout","text":"Writing mixer_wrapper.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from mixer_wrapper import MixerWrapper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T02:59:18.050364Z","iopub.execute_input":"2026-01-19T02:59:18.050648Z","execution_failed":"2026-01-19T02:59:23.525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer = st.models.Transformer(MODEL_NAME)\ntransformer.auto_model.embeddings.requires_grad_(False)\nfor param in transformer.auto_model.embeddings.parameters():\n    param.requires_grad = False\n\ntransformer.auto_model.encoder.layer[:10].requires_grad_(False)\nfor param in transformer.auto_model.encoder.layer[:10].parameters():\n    param.requires_grad = False\nEMB_SIZE = transformer.get_word_embedding_dimension()\n\npooling = st.models.Pooling(EMB_SIZE, pooling_mode=\"mean\")\nnormalization = st.models.Normalize()\nmixer_layer = MixerWrapper(EMB_SIZE, EMB_SIZE)\n\n\nmodel = st.SentenceTransformer(\n    modules=[transformer, pooling, normalization, mixer_layer, normalization],\n    device='cuda',\n    model_card_data=st.SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"SentenceTransformer model based on kuelumbus/polyBERT to predict polymeric properties\",\n        generate_widget_examples=False\n    )\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.encode(train_df.SMILES.values[:4]).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.loc[:, TARGET_COLUMNS].describe().loc[['mean', 'std']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_means = train_df.loc[:, TARGET_COLUMNS].mean().to_dict()\ntarget_means_list = [target_means[col] for col in TARGET_COLUMNS]\n# print(target_means_list)\n\nTARGET_MEANS = np.asarray(target_means_list)\nTARGET_MEANS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_deviations = train_df.loc[:, TARGET_COLUMNS].std().to_dict()\ntarget_deviations_list = [target_deviations[col] for col in TARGET_COLUMNS]\n\nTARGET_DEVIATIONS = np.asarray(target_deviations_list)\nTARGET_DEVIATIONS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.device\ntrain_df.loc[:, TARGET_COLUMNS] = ((train_df.loc[:, TARGET_COLUMNS] \n                                    - TARGET_MEANS) / TARGET_DEVIATIONS)\nval_df.loc[:, TARGET_COLUMNS] = ((val_df.loc[:, TARGET_COLUMNS] \n                                    - TARGET_MEANS) / TARGET_DEVIATIONS)\n\ntest_df.loc[:, TARGET_COLUMNS] = ((test_df.loc[:, TARGET_COLUMNS] \n                                    - TARGET_MEANS) / TARGET_DEVIATIONS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('VAL')\nprint(val_df[TARGET_COLUMNS].mean(), '\\n---\\n', val_df[TARGET_COLUMNS].std())\nprint('\\nTEST')\nprint(test_df[TARGET_COLUMNS].mean(), '\\n---\\n', test_df[TARGET_COLUMNS].std())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_augmented_data_from_index(\n        df, index, n_repeats=1, # sample_random=False, \n        target_columns=TARGET_COLUMNS,\n        random_state=None,\n        smiles_column='SMILES',\n        additional_smiles_columns=additional_smiles_columns,\n    ):\n    targets_list = df.loc[index, target_columns].values\n    if n_repeats > 1:\n        targets_list = np.repeat(targets_list, n_repeats, axis=0)\n        repeated_index = np.repeat(index, n_repeats, axis=0)\n    else:\n        repeated_index = index\n    if random_state is None:\n        # use deterministic approach\n        smiles_list1 = df.loc[repeated_index, smiles_column].values\n        smiles_column2 = additional_smiles_columns[0] if len(additional_smiles_columns) > 0 else smiles_column\n        smiles_list2 = df.loc[repeated_index, smiles_column2].values\n    else:\n        np.random.seed(random_state)\n        smiles2select = df.loc[repeated_index, [smiles_column]+additional_smiles_columns].values\n        smiles2select = [sorted(np.unique(xs)) for xs in smiles2select]\n        smiles2select = [np.random.choice(xs, 2) for xs in smiles2select]\n        smiles_list1, smiles_list2 = list(zip(*smiles2select))\n        # print(len(smiles_list1), len(smiles_list2))\n    return {\n        'smiles1': smiles_list1,\n        'smiles2': smiles_list2,\n        'label': targets_list\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\nprint(\"dataframe sizes:\", train_df.shape[0], val_df.shape[0], test_df.shape[0])\n\nnp.random.seed(42)\nTRAIN_SIZE = 100_00\nVAL_SIZE = 5000\nTEST_SIZE = 5000\n\nNUM_TRAIN_REPEATS = 5\n\ntrain_index = []\nfor col in TARGET_COLUMNS:\n    indices = np.random.choice(\n        train_df[train_df[col].isnull()].index, (TRAIN_SIZE// len(TARGET_COLUMNS),))\n    train_index.append(indices)\ntrain_index = np.concatenate(train_index)\n\nprint(train_index.shape)\n# print('smth', train_pair_index.shape)\n# train_pair_index = np.random.choice(train_df.index, (2, TRAIN_SIZE))\ntrain_dict = get_augmented_data_from_index(\n    train_df, \n    train_index, \n    n_repeats=NUM_TRAIN_REPEATS,\n    random_state=42\n)\ntrain_dataset = Dataset.from_dict(train_dict)\n\n# val_pair_index = np.random.choice(val_df.index, (2, VAL_SIZE))\nval_index = []\nfor col in TARGET_COLUMNS:\n    indices = np.random.choice(\n        val_df[val_df[col].isnull()].index, (VAL_SIZE// len(TARGET_COLUMNS), ))\n    val_index.append(indices)\nval_index = np.concatenate(val_index)\nval_dict = get_augmented_data_from_index(val_df, val_index)\nval_dataset = Dataset.from_dict(val_dict)\n\n#test_pair_index = np.random.choice(val_df.index, (2, TEST_SIZE))\ntest_index = []\nfor col in TARGET_COLUMNS:\n    indices = np.random.choice(\n        test_df[test_df[col].isnull()].index,\n        (TEST_SIZE// len(TARGET_COLUMNS), ))\n    test_index.append(indices)\ntest_index = np.concatenate(test_index)\ntest_dict = get_augmented_data_from_index(test_df, test_index)\ntest_dataset = Dataset.from_dict(test_dict)\n# 764*764/2\nprint(\"selected index sizes:\", train_index.shape, val_index.shape, test_index.shape)\n\nprint(\"final dataset sizes\", train_dataset.num_rows, val_dataset.num_rows, test_dataset.num_rows)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef normalize_embeddings(embeddings):\n    return F.normalize(embeddings, p=2, dim=1)\n\n_convert_to_tensor=st.util.tensor._convert_to_tensor\n\n\ndef pairwise_euclidean_distance(a: list | np.ndarray | Tensor, b: list | np.ndarray | Tensor):\n    a = _convert_to_tensor(a)\n    b = _convert_to_tensor(b)\n\n    return torch.sqrt(torch.sum((a - b) ** 2, dim=1)).to_dense()\n\ndef pairwise_euclidean_sim(a: list | np.ndarray | Tensor, b: list | np.ndarray | Tensor):\n    distance = pairwise_euclidean_distance(a, b)\n\n    return -distance\n\ndef pairwise_manhattan_sim(a: list | np.ndarray | Tensor, b: list | np.ndarray | Tensor):\n    a = _convert_to_tensor(a)\n    b = _convert_to_tensor(b)\n\n    return -torch.sum(torch.abs(a - b), dim=1).to_dense()\n\ndef pairwise_dot_score(a: Tensor, b: Tensor) -> Tensor:\n\n    a = _convert_to_tensor(a)\n    b = _convert_to_tensor(b)\n\n    return (a * b).sum(dim=1).to_dense()\n\ndef pairwise_cos_sim(a: Tensor, b: Tensor) -> Tensor:\n    a = st.util._convert_to_tensor(a)\n    b = _convert_to_tensor(b)\n\n    # Handle sparse tensors\n    if a.is_sparse or b.is_sparse:\n        a_norm = normalize_embeddings(a)\n        b_norm = normalize_embeddings(b)\n        return (a_norm * b_norm).sum(dim=1).to_dense()\n    else:\n        return pairwise_dot_score(normalize_embeddings(a), normalize_embeddings(b)).to_dense()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://github.com/kaiwenzha/Rank-N-Contrast/blob/main/loss.py\n# below is modified version to support multiple targets\nfrom typing import Iterable, Any\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LabelDifference(nn.Module):\n    def __init__(self, distance_type='l1'):\n        super(LabelDifference, self).__init__()\n        self.distance_type = distance_type\n\n    def forward(self, labels):\n        # labels: [bs, label_dim]\n        # output: [bs, bs]\n        if self.distance_type == 'l1':\n            return torch.abs(labels[:, None, :] - labels[None, :, :]).sum(dim=-1)\n        else:\n            raise ValueError(self.distance_type)\n\n\nclass FeatureSimilarity(nn.Module):\n    def __init__(self, similarity_type='l2'):\n        super(FeatureSimilarity, self).__init__()\n        self.similarity_type = similarity_type\n\n    def forward(self, features):\n        # labels: [bs, feat_dim]\n        # output: [bs, bs]\n        if self.similarity_type == 'l2':\n            return - (features[:, None, :] - features[None, :, :]).norm(2, dim=-1)\n        else:\n            raise ValueError(self.similarity_type)\n\n\nclass MultilabelRnCLoss(nn.Module):\n    \"\"\"\n    Each sentence transformer head returns tensor with float values [batch_size, out_embedding_size, n_channels], there is a pair of them.\n    Labels have shape [batch_size, n_channels], might also contain NaNs.\n\n    First, the loss computes euclidean distance between pair of tensors with the shape [batch_size, out_embedding_size, n_channels], along the dimension 1.\n    \"\"\"\n    def __init__(\n        self,\n        model: st.SentenceTransformer,\n        n_channels: int=5,\n        # loss_fct: nn.Module = nn.MSELoss(),\n        # score_transformation: nn.Module = nn.Identity(),\n        temperature=2, \n        label_diff='l1', \n        feature_sim='l2'\n    ) -> None:\n\n        super().__init__()\n        \n        self.model = model\n        self.n_channels = n_channels\n        # self.loss_fct = loss_fct\n        # self.score_transformation = score_transformation\n        \n        self.t = temperature\n        self.label_diff_fn = LabelDifference(label_diff)\n        self.feature_sim_fn = FeatureSimilarity(feature_sim)\n    \n\n    def forward(self, sentence_features: Iterable[dict[str, Tensor]], labels: Tensor) -> Tensor:\n        f1, f2 = [self.model(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features]\n        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n        feature_splits = torch.split(features, 1, dim=-1)\n        label_splits = torch.split(labels, 1, dim=-1)\n        loss_parts = []\n        for tgt_features, tgt_values in zip(feature_splits, label_splits):\n            # print(tgt_features.shape, tgt_values.shape)\n            tgt_features = tgt_features.squeeze(-1)\n            # tgt_values = tgt_values.squeeze(-1)\n            ids = ~torch.isnan(tgt_values).any(1)\n            # print(ids.shape, tgt_features.shape, tgt_values.shape)\n            if ids.sum() < 1:\n                continue\n            loss = self.compute_rnc_loss(tgt_features[ids], tgt_values[ids])\n            # print(loss.shape)\n            loss_parts.append(loss)\n            \n        return torch.stack(loss_parts).mean()\n        \n    def compute_rnc_loss(self, features, labels):\n        # features: [bs, 2, feat_dim]\n        # labels: [bs, label_dim]\n        # print(features.shape, labels.shape)\n\n        features = torch.cat([features[:, 0], features[:, 1]], dim=0)  # [2bs, feat_dim]\n        # print(features.device)\n        labels = labels.repeat(2, 1)  # [2bs, label_dim]\n        # print(labels.shape)\n\n        label_diffs = self.label_diff_fn(labels)\n        logits = self.feature_sim_fn(features).div(self.t)\n        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n        logits -= logits_max.detach()\n        exp_logits = logits.exp()\n\n        n = logits.shape[0]  # n = 2bs\n\n        # remove diagonal\n        logits = logits.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n        exp_logits = exp_logits.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n        label_diffs = label_diffs.masked_select((1 - torch.eye(n).to(logits.device)).bool()).view(n, n - 1)\n\n        loss = 0.\n        for k in range(n - 1):\n            pos_logits = logits[:, k]  # 2bs\n            pos_label_diffs = label_diffs[:, k]  # 2bs\n            neg_mask = (label_diffs >= pos_label_diffs.view(-1, 1)).float()  # [2bs, 2bs - 1]\n            pos_log_probs = pos_logits - torch.log((neg_mask * exp_logits).sum(dim=-1))  # 2bs\n            loss += - (pos_log_probs / (n * (n - 1))).sum()\n\n        return loss\n    # def compute_loss_from_embeddings(self, embeddings: list[Tensor], labels: Tensor) -> Tensor:\n    #     # cosine_similarities = torch.cosine_similarity(embeddings[0], embeddings[1], dim=1)\n    #     pair_distances = pairwise_euclidean_distance(embeddings[0], embeddings[1])\n    #     output = self.score_transformation(pair_distances)\n    #     # output = self.cos_score_transformation(cosine_similarities)\n    #     condition = ~torch.isnan(labels)\n    #     output = torch.where(condition, output, torch.zeros_like(output, requires_grad=False))\n    #     prepared_labels = torch.where(condition, labels, torch.zeros_like(output, requires_grad=False))\n    #     return self.loss_fct(output, prepared_labels.float())\n\n    def get_config_dict(self) -> dict[str, Any]:\n        return {\n            'temperature': self.t,\n            'label_diff_fn': st.util.misc.fullname(self.label_diff_fn), #st.util.misc.fullname(),\n            'feature_sim_fn': st.util.misc.fullname(self.feature_sim_fn),\n            #\"loss_fct\": st.util.misc.fullname(self.loss_fct)\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss = MultilabelRnCLoss(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers.training_args import BatchSamplers\n\n\nargs = st.SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"model_saves\",\n    # Optional training parameters:\n    num_train_epochs=10,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    # batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"polymers-contrastive\",  # Will be used in W&B if `wandb` is installed\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import pearsonr, spearmanr\nimport csv\n\nfrom sentence_transformers.similarity_functions import SimilarityFunction\n\nclass MultilabelSilhouetteEvaluator(st.evaluation.EmbeddingSimilarityEvaluator):\n    def __init__(self, sentences=[], scores=[], name='evaluator', num_classes=10):\n        super().__init__(sentences1=sentences, sentences2=sentences, scores=scores)\n        self.sentences = sentences\n        self.scores = scores\n        self.name = name\n        self.num_classes = num_classes\n\n    def __call__(\n        self,\n        model: st.SentenceTransformer,\n        output_path: str | None = None,\n        epoch: int = -1,\n        steps: int = -1\n    ) -> dict[str, float]:\n        if epoch != -1:\n            if steps == -1:\n                out_txt = f\" after epoch {epoch}\"\n            else:\n                out_txt = f\" in epoch {epoch} after {steps} steps\"\n        else:\n            out_txt = \"\"\n        #if self.truncate_dim is not None:\n        #    out_txt += f\" (truncated to {self.truncate_dim})\"\n\n        # logger.info(f\"EmbeddingSimilarityEvaluator: Evaluating the model on the {self.name} dataset{out_txt}:\")\n\n        embeddings = self.embed_inputs(model, self.sentences)\n        # embeddings = embeddings.detach().cpu().numpy()\n        # embeddings2 = self.embed_inputs(model, self.sentences2)\n        # print(embeddings1.shape)\n        # # Binary and ubinary embeddings are packed, so we need to unpack them for the distance metrics\n        # if self.precision == \"binary\":\n        #     embeddings1 = (embeddings1 + 128).astype(np.uint8)\n        #     embeddings2 = (embeddings2 + 128).astype(np.uint8)\n        # if self.precision in (\"ubinary\", \"binary\"):\n        #     embeddings1 = np.unpackbits(embeddings1, axis=1)\n        #     embeddings2 = np.unpackbits(embeddings2, axis=1)\n        num_scores = self.scores.shape[1]\n        scores_list = []\n        for i in range(num_scores):\n            targets = self.scores[:, i]\n            ids = ~np.isnan(targets)\n            targets = targets[ids]\n            bins = np.histogram_bin_edges(targets, self.num_classes, range=(targets.min() - 1e-3, targets.max() + 1e-3))\n            tgt_classes = np.digitize(targets, bins=bins)\n            features = embeddings[ids, :, i]\n            score = silhouette_score(features, tgt_classes)\n            scores_list.append(score)\n            \n        # labels = np.np.stack(labels, axis=-1)\n        # print(labels.shape)\n\n        metrics = {}\n        metric_column_names = []\n        for column, score in zip(TARGET_COLUMNS, scores_list):\n            # print(scores.shape, labels.shape)\n            #eval_pearson, _ = pearsonr(target, pred)\n            # eval_spearman, _ = spearmanr(target, pred)\n            # mae_values = np.abs(target - pred)\n            metrics[f\"silhouette_{column}\"] = score\n            # metrics[f\"spearman_{column}\"] = eval_spearman\n            # metrics[f\"mae_{column}\"] = mae_values.mean()\n            metric_column_names.extend([\n                f\"silhouette_{column}\",\n            ])\n        \n        metrics['silhouette_mean'] = np.mean(scores_list)\n        metric_column_names.append('silhouette_mean')\n\n        if output_path is not None and self.write_csv:\n            csv_path = os.path.join(output_path, self.csv_file)\n            output_file_exists = os.path.isfile(csv_path)\n            with open(csv_path, newline=\"\", mode=\"a\" if output_file_exists else \"w\", encoding=\"utf-8\") as f:\n                writer = csv.writer(f)\n                if not output_file_exists:\n                    writer.writerow(['epoch', 'steps'] + metric_column_names)\n\n                writer.writerow([epoch, steps] + [metrics[column] for column in metric_column_names])\n\n        # if len(self.similarity_fn_names) > 1:\n        #     metrics[\"pearson_max\"] = max(metrics[f\"pearson_{fn_name}\"] for fn_name in self.similarity_fn_names)\n        #     metrics[\"spearman_max\"] = max(metrics[f\"spearman_{fn_name}\"] for fn_name in self.similarity_fn_names)\n        self.primary_metric = 'silhouette_mean'\n\n        # if self.main_similarity:\n        #     self.primary_metric = {\n        #         SimilarityFunction.COSINE: \"spearman_cosine\",\n        #         SimilarityFunction.EUCLIDEAN: \"spearman_euclidean\",\n        #         SimilarityFunction.MANHATTAN: \"spearman_manhattan\",\n        #         SimilarityFunction.DOT_PRODUCT: \"spearman_dot\",\n        #     }.get(self.main_similarity)\n        # else:\n        #     if len(self.similarity_fn_names) > 1:\n        #         self.primary_metric = \"spearman_max\"\n        #     else:\n        #         self.primary_metric = f\"spearman_{self.similarity_fn_names[0]}\"\n        #print('before adding prefix', metrics)\n        metrics = self.prefix_name_to_metrics(metrics, self.name)\n        #print('store in model card', metrics)\n        self.store_metrics_in_model_card_data(model, metrics, epoch, steps)\n        return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_evaluator = MultilabelSilhouetteEvaluator(\n    sentences=val_dataset['smiles1'],\n    scores=np.asarray(val_dataset['label']),\n    name=\"val\",\n)\n    #anchors=val_dataset[\"anchor\"],\n#     positives=eval_dataset[\"positive\"],\n#     negatives=eval_dataset[\"negative\"],\n#     name=\"all-nli-dev\",\n# )\nprint(dev_evaluator(model))\n\n# 7. Create a trainer & train\ntrainer = st.SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    loss=train_loss,\n    evaluator=dev_evaluator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_evaluator = MultilabelSilhouetteEvaluator(\n    sentences=test_dataset[\"smiles1\"],\n    # sentences2=test_dataset[\"smiles2\"],\n    scores=np.asarray(test_dataset['label']),\n    name=\"test\",\n)\ntest_evaluator(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODELSAVEDIR = \"polymers-v1-model\"\nmodel.save_pretrained(MODELSAVEDIR)\nshutil.copy(\"mixer_wrapper.py\", MODELSAVEDIR)\n\ndel model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = st.SentenceTransformer(MODELSAVEDIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = model.encode(merged_df.SMILES.values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nembeddings2d = []\nreducers_list = []\nfor i, target_column in enumerate(TARGET_COLUMNS):\n    print(target_column)\n    train_ids = merged_df.fold_name.values=='train'\n    reducer = PCA(n_components=2)\n    reducer.fit(embeddings[train_ids, :, i])\n    e2d = reducer.transform(embeddings[:, :, i])\n    embeddings2d.append(e2d)\n    reducers_list.append(reducer)\n\nembeddings2d = np.stack(embeddings2d, axis=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i, target_column in enumerate(TARGET_COLUMNS):\n    sel_ids = ~merged_df[target_column].isnull()\n    ax = sns.scatterplot(\n        x=embeddings2d[sel_ids, 0, i], \n        y=embeddings2d[sel_ids, 1, i],\n        hue=merged_df.loc[sel_ids, target_column],\n        s=15, \n        palette='Spectral',\n        marker='.',\n        alpha=0.5,\n        legend=False\n    )\n    ax.set_title(target_column)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}